{"cells":[{"cell_type":"markdown","metadata":{"id":"jLKUdvI9W93R"},"source":["You will be performing data exploration and data preparation steps gradually on the dataset you have chosen in Assignment 1:\n","\n","1.For each column in the dataset:\n","\n","a. (Using Python) Develop the module that identifies :\n","\n","i. missing data (7 pts)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1088,"status":"ok","timestamp":1698950947068,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"sMIxBCp_XT4F","outputId":"5653530e-1797-478f-aecd-5921f2934f87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Columns with missing data:\n","Series([], dtype: int64)\n"]}],"source":["\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n","column_names = [\n","    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n","    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n","]\n","data = pd.read_csv(url, names=column_names)\n","missing_data = data.isnull().sum()\n","print(\"Columns with missing data:\")\n","print(missing_data[missing_data > 0])\n","# The output shows that there is no missing data in this dataset .\n","# Since this dataset is cleaned from a website like kaggle \n","# If there was a missing data I should detect the columns with a single value or columns having a few single values or columns having low variance \n","# we should delete these columns by delete and drop coding in pyhton .\n","# If there is a duplicate columns and rows \n","# we should delete these duplicted columns and rows drop duplicates in python \n","# we can use threshold variance to clean missing data \n","# you can count missing data and replace them by nan \n","# we can remove 4 columns out of 14 if these columns are not important and do not affect the machine learning and performance of the model and that \n","# is to reduce the number of features to the required number which is 10 distributed between categorical and numerical data ."]},{"cell_type":"markdown","metadata":{"id":"0rgRPHyPYaI7"},"source":["ii. outlier data (7 pts)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":853,"status":"ok","timestamp":1698952328654,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"11i2bRyvV5-7","outputId":"8fea14df-b8a5-4779-841d-1168d3a30e6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Outlier data:\n","      age  trestbps   chol  thalach  oldpeak\n","48   65.0     140.0  417.0    157.0      0.8\n","91   62.0     160.0  164.0    145.0      6.2\n","121  63.0     150.0  407.0    154.0      4.0\n","123  55.0     140.0  217.0    111.0      5.6\n","126  56.0     200.0  288.0    133.0      4.0\n","152  67.0     115.0  564.0    160.0      1.6\n","181  56.0     134.0  409.0    150.0      1.9\n","188  54.0     192.0  283.0    195.0      0.0\n","245  67.0     120.0  237.0     71.0      1.0\n"]}],"source":["\n","continuous_columns = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n","z_scores = np.abs((data[continuous_columns] - data[continuous_columns].mean()) / data[continuous_columns].std())\n","outlier_threshold = 3\n","outliers = z_scores > outlier_threshold\n","print(\"Outlier data:\")\n","print(data[outliers.any(axis=1)][continuous_columns])\n","# This output shows the existence of outliers in this dataset .\n","# use z-score ,threshold ,to remove outliers \n","# we can use IQR or automatic outlier detection MAE (no normal distribution )\n","# we can use combination of these \n","# The only important thing is to not remove an outlier if it was an important point for this data .\n","# you can ask expert "]},{"cell_type":"markdown","metadata":{"id":"UrY-nDL5djZp"},"source":["iii. anomalous data (7 pts)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1698952620145,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"tSUouGAdV7iv","outputId":"f1230d7e-ee6f-47f2-d416-5299c10b0ab3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Anomalous data:\n","Empty DataFrame\n","Columns: [age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, target]\n","Index: []\n"]}],"source":["anomalous_criteria = data[\"age\"] > 100\n","print(\"Anomalous data:\")\n","print(data[anomalous_criteria])\n","# when doing anamolous criteria for age we didn't detect an age graeter than 100 \n","# So for age we havo anomalous data."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1698955040818,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"TmNpBiAPnCh2","outputId":"14cb4369-8e46-4568-f5f2-f3641659c698"},"outputs":[{"name":"stdout","output_type":"stream","text":["Anomalous Cholesterol Data:\n","      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n","7    57.0  0.0  4.0     120.0  354.0  0.0      0.0    163.0    1.0      0.6   \n","38   55.0  1.0  4.0     132.0  353.0  0.0      0.0    132.0    1.0      1.2   \n","48   65.0  0.0  3.0     140.0  417.0  1.0      2.0    157.0    0.0      0.8   \n","75   65.0  0.0  3.0     160.0  360.0  0.0      2.0    151.0    0.0      0.8   \n","93   44.0  0.0  3.0     108.0  141.0  0.0      0.0    175.0    0.0      0.6   \n","121  63.0  0.0  4.0     150.0  407.0  0.0      2.0    154.0    0.0      4.0   \n","152  67.0  0.0  3.0     115.0  564.0  0.0      2.0    160.0    0.0      1.6   \n","173  62.0  0.0  4.0     140.0  394.0  0.0      2.0    157.0    0.0      1.2   \n","181  56.0  0.0  4.0     134.0  409.0  0.0      2.0    150.0    1.0      1.9   \n","202  57.0  1.0  3.0     150.0  126.0  1.0      0.0    173.0    0.0      0.2   \n","300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n","\n","     slope   ca thal  target  \n","7      1.0  0.0  3.0       0  \n","38     2.0  1.0  7.0       3  \n","48     1.0  1.0  3.0       0  \n","75     1.0  0.0  3.0       0  \n","93     2.0  0.0  3.0       0  \n","121    2.0  3.0  7.0       4  \n","152    2.0  0.0  7.0       0  \n","173    2.0  0.0  3.0       0  \n","181    2.0  2.0  7.0       2  \n","202    1.0  1.0  7.0       0  \n","300    2.0  1.0  7.0       3  \n"]}],"source":["z_score_threshold = 2\n","z_scores_chol = (data['chol'] - data['chol'].mean()) / data['chol'].std()\n","anomalous_chol = data[z_scores_chol.abs() > z_score_threshold]\n","print(\"Anomalous Cholesterol Data:\")\n","print(anomalous_chol)\n","# when doing anomalous criteria for cholesterol we have anomalous data \n","# there are values in cholesterol column that are not satisfying the condition of z-score-threshold "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1698955183595,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"y_QoPvHpnZAk","outputId":"99004516-0a47-4587-b046-0d54d3554c96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Anomalous Resting Blood Pressure Data:\n","      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n","14   52.0  1.0  3.0     172.0  199.0  1.0      0.0    162.0    0.0      0.5   \n","68   59.0  1.0  4.0     170.0  326.0  0.0      2.0    140.0    1.0      3.4   \n","83   68.0  1.0  3.0     180.0  274.0  1.0      2.0    150.0    1.0      1.6   \n","126  56.0  0.0  4.0     200.0  288.0  1.0      2.0    133.0    1.0      4.0   \n","131  51.0  1.0  3.0      94.0  227.0  0.0      0.0    154.0    1.0      0.0   \n","141  59.0  1.0  1.0     170.0  288.0  0.0      2.0    159.0    0.0      0.2   \n","172  59.0  0.0  4.0     174.0  249.0  0.0      0.0    143.0    1.0      0.0   \n","183  59.0  1.0  1.0     178.0  270.0  0.0      2.0    145.0    0.0      4.2   \n","188  54.0  1.0  2.0     192.0  283.0  0.0      2.0    195.0    0.0      0.0   \n","201  64.0  0.0  4.0     180.0  325.0  0.0      0.0    154.0    1.0      0.0   \n","213  66.0  0.0  4.0     178.0  228.0  1.0      0.0    165.0    1.0      1.0   \n","222  39.0  0.0  3.0      94.0  199.0  0.0      0.0    179.0    0.0      0.0   \n","231  55.0  0.0  4.0     180.0  327.0  0.0      1.0    117.0    1.0      3.4   \n","275  64.0  1.0  1.0     170.0  227.0  0.0      2.0    155.0    0.0      0.6   \n","286  58.0  0.0  4.0     170.0  225.0  1.0      2.0    146.0    1.0      2.8   \n","\n","     slope   ca thal  target  \n","14     1.0  0.0  7.0       0  \n","68     3.0  0.0  7.0       2  \n","83     2.0  0.0  7.0       3  \n","126    3.0  2.0  7.0       3  \n","131    1.0  1.0  7.0       0  \n","141    2.0  0.0  7.0       1  \n","172    2.0  0.0  3.0       1  \n","183    3.0  0.0  7.0       0  \n","188    1.0  1.0  7.0       1  \n","201    1.0  0.0  3.0       0  \n","213    2.0  2.0  7.0       3  \n","222    1.0  0.0  3.0       0  \n","231    2.0  0.0  3.0       2  \n","275    2.0  0.0  7.0       0  \n","286    2.0  2.0  6.0       2  \n"]}],"source":["z_score_threshold = 2\n","z_scores_trestbps = (data['trestbps'] - data['trestbps'].mean()) / data['trestbps'].std()\n","anomalous_trestbps = data[z_scores_trestbps.abs() > z_score_threshold]\n","print(\"Anomalous Resting Blood Pressure Data:\")\n","print(anomalous_trestbps)\n","# This code shoed the anomalous on trestbps \n","# there are values in tresbps columns that do not satisfy the condition of z-score-threshold "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1698955243110,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"Qfk-9eFanvBn","outputId":"ed6a528f-a05c-41db-a383-c8e188901930"},"outputs":[{"name":"stdout","output_type":"stream","text":["Anomalous Maximum Heart Rate Data:\n","      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n","72   62.0  1.0  4.0     120.0  267.0  0.0      0.0     99.0    1.0      1.8   \n","114  62.0  0.0  3.0     130.0  263.0  0.0      0.0     97.0    0.0      1.2   \n","132  29.0  1.0  2.0     130.0  204.0  0.0      2.0    202.0    0.0      0.0   \n","137  62.0  1.0  2.0     120.0  281.0  0.0      2.0    103.0    0.0      1.4   \n","154  64.0  1.0  4.0     120.0  246.0  0.0      2.0     96.0    1.0      2.2   \n","175  57.0  1.0  4.0     152.0  274.0  0.0      0.0     88.0    1.0      1.2   \n","223  53.0  1.0  4.0     123.0  282.0  0.0      0.0     95.0    1.0      2.0   \n","236  56.0  1.0  4.0     130.0  283.0  1.0      2.0    103.0    1.0      1.6   \n","244  60.0  0.0  3.0     120.0  178.0  1.0      0.0     96.0    0.0      0.0   \n","245  67.0  1.0  4.0     120.0  237.0  0.0      0.0     71.0    0.0      1.0   \n","296  59.0  1.0  4.0     164.0  176.0  1.0      2.0     90.0    0.0      1.0   \n","\n","     slope   ca thal  target  \n","72     2.0  2.0  7.0       1  \n","114    2.0  1.0  7.0       2  \n","132    1.0  0.0  3.0       0  \n","137    2.0  1.0  7.0       3  \n","154    3.0  1.0  3.0       3  \n","175    2.0  1.0  7.0       1  \n","223    2.0  2.0  7.0       3  \n","236    3.0  0.0  7.0       2  \n","244    1.0  0.0  3.0       0  \n","245    2.0  0.0  3.0       2  \n","296    2.0  2.0  6.0       3  \n"]}],"source":["z_score_threshold = 2\n","z_scores_thalach = (data['thalach'] - data['thalach'].mean()) / data['thalach'].std()\n","anomalous_thalach = data[z_scores_thalach.abs() > z_score_threshold]\n","print(\"Anomalous Maximum Heart Rate Data:\")\n","print(anomalous_thalach)\n","# This code showed anomalous in thalach\n","# It does not satisfy the z-score threshold condition "]},{"cell_type":"markdown","metadata":{"id":"723HiVuwfCAd"},"source":["b. Explain the rationale behind the methods used for identifying:\n","\n","i. outliers (7 pts)"]},{"cell_type":"markdown","metadata":{"id":"IBom7vBBgU0L"},"source":["\n","Identifying Outliers:\n","\n","1.\tZ-Score Method:\n","\n","•\tRationale: The Z-Score method is based on the idea that data points significantly far from the mean are likely to be outliers. It measures how many    standard deviations a data point is away from the mean.\n","\n","•\tMethod: Calculate the Z-Score for each data point in a continuous variable. Data points with Z-Scores beyond a certain threshold (e.g., 3) are considered outliers.\n","\n","•\tPros:\n","\n","•\tWell-established and widely used method.\n","\n","•\tIt's data distribution-agnostic and works well with normally distributed and non-normally distributed data.\n","\n","•\tCons:\n","\n","•\tSensitive to extreme values, which might incorrectly label certain values as outliers.\n"]},{"cell_type":"markdown","metadata":{"id":"fZHqcW2kgjfU"},"source":["ii. anomalous data (7 pts)"]},{"cell_type":"markdown","metadata":{"id":"GxpQXrQ-gkU8"},"source":["Identifying Anomalous Data:\n","\n","2.\tCustom Criteria Method:\n","\n","•\tRationale: Identifying anomalous data often depends on domain-specific knowledge and use case requirements. Anomalies are not necessarily outliers but can be any data point that doesn't conform to expected patterns or criteria.\n","\n","•\tMethod: Define specific criteria that, based on domain knowledge, make data points anomalous. For example, in the Heart Disease UCI dataset:\n","•\tA patient with an \"age\" greater than 100 might be considered anomalous.\n","•\tA \"thalach\" (maximum heart rate) below a certain threshold could also be considered anomalous.\n","\n","•\tPros:\n","\n","•\tFlexibility to customize anomaly detection based on domain expertise.\n","\n","•\tCan capture anomalies that wouldn't be detected by statistical methods.\n","\n","•\tCons:\n","\n","•\tSubjective and dependent on domain knowledge.\n","\n","•\tMight not generalize well to other datasets or domains.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Dw1XQCe7gtxT"},"source":["2. For each column in the dataset:\n","\n","a. (Using Python) Develop the module that imputes missing or anomalous values when needed in two different ways. (7 pts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1698958637232,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"u4O4CMmWV8mK","outputId":"34ddc5c2-1a36-419c-a47f-9a458c7b46cc"},"outputs":[],"source":["\n","\n","categorical_columns = ['ca', 'thal']\n","for column in categorical_columns:\n","    mode_value = data[column].mode()[0]\n","    data[column].fillna(mode_value, inplace=True)\n","    print(mode_value)\n","    print(data[column].fillna(mode_value, inplace=True))\n","   # print(data)\n"]},{"cell_type":"markdown","metadata":{},"source":["0.0\n","None\n","3.0\n","None"]},{"cell_type":"markdown","metadata":{"id":"kLW8A2gE1LJi"},"source":["The second code will outputs the imputed data for numerical columns by 2 ways median and mean as a second way ."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1698959310770,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"sTBdGLeC1bsr","outputId":"22cd4129-0c21-4fb3-e908-db04e4453dd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["      age  trestbps   chol  thalach  oldpeak\n","0    63.0     145.0  233.0    150.0      2.3\n","1    67.0     160.0  286.0    108.0      1.5\n","2    67.0     120.0  229.0    129.0      2.6\n","3    37.0     130.0  250.0    187.0      3.5\n","4    41.0     130.0  204.0    172.0      1.4\n","..    ...       ...    ...      ...      ...\n","298  45.0     110.0  264.0    132.0      1.2\n","299  68.0     144.0  193.0    141.0      3.4\n","300  57.0     130.0  131.0    115.0      1.2\n","301  57.0     130.0  236.0    174.0      0.0\n","302  38.0     138.0  175.0    173.0      0.0\n","\n","[303 rows x 5 columns]\n"]}],"source":["\n","from sklearn.impute import SimpleImputer\n","numerical_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']  \n","imputer = SimpleImputer(strategy='mean')\n","data[numerical_columns] = imputer.fit_transform(data[numerical_columns])\n","print(data[numerical_columns] )"]},{"cell_type":"markdown","metadata":{"id":"-HE5r1oK3tWh"},"source":["The second way is median imputer for the numerical columns in this data ."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1063,"status":"ok","timestamp":1698959511839,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"2YNqUW_C34r1","outputId":"131e5e23-237b-4da9-e84f-3ce36ec9e821"},"outputs":[{"name":"stdout","output_type":"stream","text":["      age  trestbps   chol  thalach  oldpeak\n","0    63.0     145.0  233.0    150.0      2.3\n","1    67.0     160.0  286.0    108.0      1.5\n","2    67.0     120.0  229.0    129.0      2.6\n","3    37.0     130.0  250.0    187.0      3.5\n","4    41.0     130.0  204.0    172.0      1.4\n","..    ...       ...    ...      ...      ...\n","298  45.0     110.0  264.0    132.0      1.2\n","299  68.0     144.0  193.0    141.0      3.4\n","300  57.0     130.0  131.0    115.0      1.2\n","301  57.0     130.0  236.0    174.0      0.0\n","302  38.0     138.0  175.0    173.0      0.0\n","\n","[303 rows x 5 columns]\n"]}],"source":["\n","numerical_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']  \n","imputer = SimpleImputer(strategy='median')\n","data[numerical_columns] = imputer.fit_transform(data[numerical_columns])\n","print(data[numerical_columns])\n"]},{"cell_type":"markdown","metadata":{"id":"R5yvfWja4SrA"},"source":["b. Explain the rationale behind the methods used for imputing the data (e.g., based on data types, data distribution, etc.). (7 pts)"]},{"cell_type":"markdown","metadata":{"id":"OopY7bX86Pi1"},"source":["The choice of imputation methods for categorical and numerical data in the Heart Disease UCI dataset is based on various factors, including data types, data distribution, and the characteristics of the dataset. Here's the rationale behind using mode imputation for categorical data and median or mean imputation for numerical data:\n","\n","Mode Imputation for Categorical Data:\n","\n","Data Type: Categorical data consists of discrete categories or labels, which are not inherently ordered. The mode represents the most frequently occurring category in the data, making it a natural choice for imputation in categorical data.\n","\n","Data Distribution: Mode imputation is well-suited for categorical data with a skewed distribution, as it captures the most prevalent category, aligning with the distribution's mode.\n","\n","Data Characteristics: Categorical features often have a clear mode that represents the most common category. Imputing missing values with the mode preserves the distribution's central tendency and is a reasonable approach.\n","\n","Interpretability: Imputing with the mode maintains the interpretability of the data, as it uses actual category labels without introducing any artificial values.\n","\n","Median or Mean Imputation for Numerical Data:\n","\n","Data Type: Numerical data consists of continuous or discrete numeric values, which can be ordered and have a clear central tendency. Median and mean are measures of central tendency and are appropriate for imputing numerical data.\n","\n","Data Distribution: Median and mean imputation are suitable for numerical data with different types of distributions. Median is robust to outliers and can be preferable for skewed distributions, while mean is appropriate for normally distributed data.\n","\n","Data Characteristics: Imputing with the median or mean is a straightforward method that can be applied to numerical features with or without outliers.\n","\n","Flexibility: Depending on the data's distribution and sensitivity to outliers, you can choose between median or mean imputation. Median is more robust to extreme values and is preferred when outliers are present, while mean is used for normally distributed data.\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jjqVpJK26R2g"},"source":["3. Describe ways that your dataset can be aggregated across multiple features in meaningful and interesting ways (7 pts), and explain how that may be helpful for future (causal or predictive) analysis on the dataset. (7 pts)"]},{"cell_type":"markdown","metadata":{"id":"A_dmlS8Y6d4k"},"source":["Aggregating data across multiple features is a valuable technique for gaining insights and preparing a dataset for various types of analysis, including causal or predictive analysis. In the context of the \"Heart Disease UCI\" dataset, here are some meaningful and interesting ways to aggregate the data, along with explanations of their potential benefits for future analysis:\n","1.\tAge Groups and Gender-Specific Aggregation:\n","•\tAggregation: Group individuals into age ranges (e.g., 20-30, 31-40, etc.) and aggregate data within each group. You can also create separate aggregates for different genders.\n","•\tBenefits: This allows you to analyze how heart disease prevalence or risk factors vary across different age groups and between genders. It's essential for understanding if certain age groups or genders are more susceptible to heart disease.\n","2.\tAggregation by Risk Factors (e.g., Smoking, Hypertension, Diabetes):\n","•\tAggregation: Aggregate data based on the presence or absence of risk factors. For example, create separate datasets for smokers and non-smokers, hypertensive and non-hypertensive individuals, and diabetics and non-diabetics.\n","•\tBenefits: This helps identify the impact of specific risk factors on heart disease. You can compare the prevalence and severity of heart disease between individuals with and without these risk factors, which is crucial for causal and predictive analysis.\n","3.\tAggregation by Exercise and Diet Habits:\n","•\tAggregation: Group individuals based on their exercise habits and diet (e.g., sedentary vs. active, healthy vs. unhealthy diet).\n","•\tBenefits: You can explore the relationship between lifestyle choices and heart disease. This can be useful for predictive modeling to assess how lifestyle changes may influence the likelihood of developing heart disease.\n","4.\tAggregation by Chest Pain Type:\n","•\tAggregation: Group data based on the type of chest pain (e.g., typical angina, atypical angina, non-anginal pain, asymptomatic).\n","•\tBenefits: This aggregation allows you to investigate the impact of chest pain type on heart disease. It's relevant for predictive modeling and understanding causal relationships.\n","5.\tTemporal Aggregation (e.g., Yearly, Monthly):\n","•\tAggregation: Aggregate data over time to analyze how heart disease prevalence or risk factors change over months or years.\n","•\tBenefits: Temporal analysis helps identify trends and seasonal variations in heart disease. It's valuable for long-term predictive analysis and assessing causal factors that may change over time.\n","6.\tGeographical Aggregation:\n","•\tAggregation: Aggregate data by region or location, if such information is available.\n","•\tBenefits: This allows you to explore regional variations in heart disease. It's useful for public health planning and can provide insights into environmental or lifestyle factors contributing to heart disease.\n","7.\tComorbidity Aggregation:\n","•\tAggregation: Create aggregates based on comorbid conditions (e.g., heart disease with diabetes, hypertension, or obesity).\n","•\tBenefits: Analyzing comorbid conditions can help in predicting the risk of heart disease in individuals with multiple health issues. It's relevant for causal analysis to understand how different conditions interact.\n","Aggregating the dataset in these ways provides a more nuanced understanding of the relationships between various factors and heart disease. It can be helpful for both causal analysis, where you want to determine causative factors, and predictive analysis, where you build models to predict the likelihood of heart disease in individuals based on their characteristics. Additionally, these aggregates can inform healthcare policies, interventions, and personalized recommendations for preventing heart disease."]},{"cell_type":"markdown","metadata":{"id":"p8BZ_JY39Erk"},"source":["4. (Using R): Develop a module that aggregates the data as you described in (3) above.  (7 pts)"]},{"cell_type":"markdown","metadata":{"id":"oRXAbu7S_7LB"},"source":["I will provide this code on RMD and HTML seperately .Output of this Rcode will be diplayed on Rmd and a microsoft . "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AWGwXc59NTF"},"outputs":[],"source":["\n","url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n","heart_data <- read.csv(url, header = FALSE)\n","\n","column_names <- c(\n","  \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n","  \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",")\n","colnames(heart_data) <- column_names\n","library(dplyr)\n","age_gender_aggregation <- heart_data %>%\n","  mutate(age_group = case_when(\n","    age >= 20 & age <= 30 ~ \"20-30\",\n","    age > 30 & age <= 40 ~ \"31-40\",\n","    # Add more age group conditions as needed\n","    TRUE ~ \"Other\"\n","  )) %>%\n","  group_by(age_group, sex) %>%\n","  summarise(\n","    mean_chol = mean(chol, na.rm = TRUE),\n","    mean_bp = mean(trestbps, na.rm = TRUE),\n","    # Add more summary statistics as needed\n","    count = n()\n","  ) %>%\n","  ungroup()\n","\n","print(age_gender_aggregation)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GCEs5tTqAKey"},"source":["5. (Using R): Develop the module that visualizes those aggregations in all interesting ways possible. Use stacking to a large extent possible to allow for overlaid, multidimensional analyses. (7 pts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoUZ-9uuAT7i"},"outputs":[],"source":["library(ggplot2)\n","library(dplyr)\n","library(GGally)\n","suppressPackageStartupMessages(library(cowplot))\n","\n","# Data preprocessing\n","heart_data <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", header = FALSE)\n","column_names <- c(\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\")\n","colnames(heart_data) <- column_names\n","\n","# Create the age_group variable in heart_data\n","heart_data <- heart_data %>%\n","  mutate(age_group = case_when(\n","    age >= 20 & age <= 30 ~ \"20-30\",\n","    age > 30 & age <= 40 ~ \"31-40\",\n","    TRUE ~ \"Other\"\n","  ))\n","\n","age_gender_aggregation <- heart_data %>%\n","  group_by(age_group, sex) %>%\n","  summarise(\n","    mean_chol = mean(chol, na.rm = TRUE),\n","    mean_bp = mean(trestbps, na.rm = TRUE),\n","    count = n()\n","  ) %>%\n","  ungroup()\n","\n","# Create individual plots\n","age_gender_plot <- ggplot(age_gender_aggregation, aes(x = age_group, y = count, fill = sex)) +\n","  geom_bar(stat = \"identity\") +\n","  labs(title = \"Heart Disease Counts by Age Group and Gender\", x = \"Age Group\", y = \"Count\") +\n","  theme_minimal()\n","\n","overlay_age_histogram <- ggplot(heart_data, aes(x = age, fill = sex)) +\n","  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.5) +\n","  labs(title = \"Overlayed Age Distribution by Gender\", x = \"Age\", y = \"Count\") +\n","  theme_minimal()\n","\n","overlay_density_plot <- ggplot(heart_data, aes(x = chol, fill = age_group)) +\n","  geom_density(alpha = 0.5) +\n","  labs(title = \"Overlayed Cholesterol Density by Age Group\", x = \"Cholesterol\", y = \"Density\") +\n","  theme_minimal()\n","\n","scatter_matrix <- ggpairs(heart_data, columns = c(\"age\", \"chol\", \"thalach\", \"trestbps\"),\n","                          mapping = aes(color = target), title = \"Scatter Plot Matrix\")\n","\n","# Create the final layout\n","final_plot <- plot_grid(\n","  plot_grid(age_gender_plot, ncol = 3),\n","  overlay_age_histogram, overlay_density_plot, scatter_matrix,\n","  nrow = 2, ncol = 2\n",")\n","\n","# Display the final plot\n","print(final_plot)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IxpXKsqwA2Bq"},"source":["6. For each column in the dataset:\n","\n","a. (Using Python) Develop a module that implements ways by which this column can be engineered in a manner that exposes clearer trends in the data, and that abides by requirements for the various ML models. (7 pts)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":804,"status":"ok","timestamp":1698963137789,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"xG1HNrQ2BIgr","outputId":"5f38a770-ba5b-4489-c6b4-92be26312aeb"},"outputs":[{"name":"stdout","output_type":"stream","text":["     sex  cp  trestbps  fbs  restecg  exang   oldpeak  slope  ca  thal  \\\n","0      1   0  0.757525    1        2      0  1.087338      2   0     1   \n","1      1   3  1.611220    0        2      1  0.397182      1   3     0   \n","2      1   3 -0.665300    0        2      1  1.346147      1   2     2   \n","3      1   2 -0.096170    0        0      0  2.122573      2   0     0   \n","4      0   1 -0.096170    0        2      0  0.310912      0   0     0   \n","..   ...  ..       ...  ...      ...    ...       ...    ...  ..   ...   \n","298    1   0 -1.234430    0        0      0  0.138373      1   0     2   \n","299    1   3  0.700612    1        0      0  2.036303      1   2     2   \n","300    1   3 -0.096170    0        0      1  0.138373      1   1     2   \n","301    0   1 -0.096170    0        2      0 -0.896862      1   1     0   \n","302    1   2  0.359134    0        0      0 -0.896862      0   4     0   \n","\n","     target  age_chol_interaction  max_hr_minus_age age_group  \n","0         0             -0.251318         -0.931529       NaN  \n","1         2              1.058499         -3.213907       NaN  \n","2         1             -0.476458         -2.294356       NaN  \n","3         0             -0.123635          3.569923       NaN  \n","4         0              1.230036          2.469825       NaN  \n","..      ...                   ...               ...       ...  \n","298       1             -0.350218          0.275023       NaN  \n","299       2             -1.561015         -1.879717       NaN  \n","300       3             -0.635216         -1.799202       NaN  \n","301       1             -0.058711          0.784300       NaN  \n","302       0              2.526658          2.846070       NaN  \n","\n","[303 rows x 14 columns]\n","X_train shape: (242, 13)\n","X_test shape: (61, 13)\n","y_train shape: (242,)\n","y_test shape: (61,)\n"]}],"source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","data = pd.read_csv(url, names=column_names)\n","categorical_columns = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]\n","\n","for col in categorical_columns:\n","    le = LabelEncoder()\n","    data[col] = le.fit_transform(data[col])\n","\n","continuous_columns = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n","\n","scaler = StandardScaler()\n","data[continuous_columns] = scaler.fit_transform(data[continuous_columns])\n","data[\"age_chol_interaction\"] = data[\"age\"] * data[\"chol\"]\n","data[\"max_hr_minus_age\"] = data[\"thalach\"] - data[\"age\"]\n","age_bins = [29, 39, 49, 59, 69, 79]\n","data[\"age_group\"] = pd.cut(data[\"age\"], bins=age_bins, labels=[\"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\"])\n","data = data.drop(columns=[\"age\", \"chol\", \"thalach\"])\n","X = data.drop(columns=[\"target\"])\n","y = data[\"target\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","print(data)\n","print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"2DItQptsG4Jp"},"source":["b. When working on (6.a) above, describe your choice of feature engineering per column, whilst making the distinction between categorical and numerical columns, as well as to choose feature engineering strategies that are justified by the underlying data distributions when applicable, or feature interactions. (7 pts)"]},{"cell_type":"markdown","metadata":{"id":"u51Ym6p4GH38"},"source":["When performing feature engineering for each column in the \"Heart Disease UCI\" dataset, it's important to consider the data distributions and the nature of the columns. Feature engineering choices can vary depending on whether the columns are categorical or numerical and should be justified based on the data's characteristics. Here's a detailed explanation of the feature engineering strategies chosen in (6.a) for each column:\n","Categorical Columns:\n","1.\tSex (Categorical - Binary): Since the \"sex\" column is binary (0 for female, 1 for male), there is no need for further engineering. The label encoding suffices to make it compatible with machine learning models.\n","2.\tChest Pain Type (Categorical - Ordinal): The \"cp\" column represents different types of chest pain and is ordinal. The label encoding provides a meaningful transformation as the values represent increasing levels of chest pain severity.\n","3.\tFasting Blood Sugar (Categorical - Binary): The \"fbs\" column indicates whether the fasting blood sugar is greater than 120 mg/dl (1 for yes, 0 for no). Label encoding is suitable for this binary categorical variable.\n","4.\tResting ECG (Categorical - Ordinal): The \"restecg\" column describes different resting electrocardiographic results. Label encoding is used, and since the values are already ordinal (0, 1, 2), this preserves their order.\n","5.\tExercise-Induced Angina (Categorical - Binary): The \"exang\" column represents the presence or absence of exercise-induced angina (1 for yes, 0 for no). Label encoding is appropriate for this binary categorical variable.\n","6.\tSlope (Categorical - Ordinal): The \"slope\" column describes the slope of the peak exercise ST segment. Label encoding is chosen, as the values are ordinal (0, 1, 2), indicating increasing degrees of slope.\n","7.\tNumber of Major Vessels Colored by Fluoroscopy (Categorical - Ordinal): The \"ca\" column represents the number of major vessels colored by fluoroscopy. It is ordinal in nature, and label encoding preserves the order of the categories.\n","8.\tThallium Stress Test Result (Categorical - Ordinal): The \"thal\" column describes the thallium stress test result. Label encoding is appropriate as the values are ordinal (0, 1, 2, 3), indicating different outcomes.\n","9.\tTarget (Categorical - Binary): The \"target\" column represents the presence or absence of heart disease (1 for presence, 0 for absence). Label encoding is used for this binary categorical target variable.\n","Numerical Columns:\n","1.\tAge (Numerical): The \"age\" column is continuous. It is standardized (scaled) using StandardScaler to ensure that it has a mean of 0 and a standard deviation of 1, making it suitable for models that are sensitive to feature scales.\n","2.\tResting Blood Pressure (Numerical): Similar to \"age,\" the \"trestbps\" column is continuous and standardized using StandardScaler.\n","3.\tSerum Cholesterol Level (Numerical): The \"chol\" column, being a continuous variable, is also standardized to have a mean of 0 and a standard deviation of 1.\n","4.\tMaximum Heart Rate (Numerical): The \"thalach\" column, representing maximum heart rate achieved, is standardized for the same reasons as blood pressure and cholesterol levels.\n","5.\tOldpeak (Numerical): The \"oldpeak\" column, indicating the ST depression induced by exercise relative to rest, is not further engineered in this example.\n","Creating Interaction Features:\n","1.\tAge and Cholesterol Interaction (Numerical): By multiplying \"age\" and \"chol,\" an interaction feature is created. This can capture the combined effect of age and cholesterol on heart disease risk.\n","2.\tMaximum Heart Rate Minus Age (Numerical): Subtracting \"age\" from \"thalach\" creates another interaction feature, which can represent the difference between age and maximum heart rate. This may capture age-related heart rate variations.\n","Binning Continuous Variables:\n","1.\tAge (Categorical): The \"age\" column is binned into categories based on age ranges. This simplifies the analysis by converting a continuous variable into categorical form and helps capture age-related trends.\n","The chosen feature engineering strategies aim to make the data more suitable for machine learning models while preserving the inherent characteristics of the variables, such as their ordinal nature. The creation of interaction features can expose potential non-linear relationships in the data. Binning age allows for the exploration of age-related trends. These strategies are chosen with careful consideration of the data distributions and the requirements of machine learning models."]},{"cell_type":"markdown","metadata":{"id":"_4dkdVmeHUFv"},"source":["7. (Using Python): Implement two feature selection methods on your “cleaned” and “engineered” dataset. At this stage, pay special attention if the data requires further processing (e.g., scaling). (10 pts: 5 pts for each method)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":279,"status":"error","timestamp":1698965378864,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"3Eaov_cXHZey","outputId":"ffa3d02c-9af0-4bbf-cd88-0892821dcdbf"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-0a3df895c1f2>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features_to_select\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_rfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mselected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfeature_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input contains NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# We need only consider float arrays, hence can early return for all else.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input contains NaN"]}],"source":["#Feature Selection using Recursive Feature Elimination (RFE)\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","X = data.drop(columns=[\"target\"])\n","y = data[\"target\"]\n","rfe = RFE(model, n_features_to_select=5)\n","X_rfe = rfe.fit_transform(X, y)\n","selected_features = rfe.support_\n","feature_ranking = rfe.ranking_\n","print(\"Selected features:\", selected_features)\n","print(\"Feature rankings:\", feature_ranking)\n"]},{"cell_type":"markdown","metadata":{"id":"KftUk_MeOoFo"},"source":["Feature Selection using Recursive Feature Elimination (RFE) shows that input contains NaN ,which implies the existence of missing values .So, my data requires further processing ."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"executionInfo":{"elapsed":268,"status":"error","timestamp":1698965054606,"user":{"displayName":"zamzam atwi","userId":"05836396607331812705"},"user_tz":-120},"id":"ldvTvgn0LrNv","outputId":"1fed5254-7e65-47e7-e617-66d0f0c399d1"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-63433665a3a3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature importances:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectFromModel\n","model = RandomForestClassifier()\n","X = data.drop(columns=[\"target\"])\n","y = data[\"target\"]\n","model.fit(X, y)\n","feature_importances = model.feature_importances_\n","print(\"Feature importances:\", feature_importances)\n","sfm = SelectFromModel(model, threshold=-np.inf, max_features=n)\n","X_new = sfm.transform(X)\n","print(\"Selected features (X_new):\")\n","print(X_new)\n"]},{"cell_type":"markdown","metadata":{"id":"-cbm4fxlNgis"},"source":["Since the output shows error ,this means my input X contains NaN ,which implies the existence of missing values .So, my data requires further processing ."]},{"cell_type":"markdown","metadata":{"id":"OCx4kVnwKMOu"},"source":["8. Elaborate whether the rankings of important features produced by those two methods align with what we know about the problem you are tackling in your chosen dataset. Those interpretations can be standalone interpretations (e.g., does common sense or domain expertise confirm that a given feature is important), or coupled interpretations that align with the EDA performed in parts 3 and 4 above. (14 pts: 7 pts for each method)"]},{"cell_type":"markdown","metadata":{"id":"HXCeMXNHKX1z"},"source":["Interpreting the rankings of important features produced by feature selection methods in the context of your specific dataset is crucial. The alignment between the selected features and domain knowledge or exploratory data analysis (EDA) can provide valuable insights into the model's feature selection process. Let's discuss the interpretations for each method:\n","1.\tRecursive Feature Elimination (RFE): RFE ranks features based on their importance in the context of the selected machine learning model. The rankings might align with domain knowledge or EDA in the following ways:\n","•\tDomain Knowledge Confirmation: If domain knowledge suggests that certain features are expected to have a strong influence on the target variable, and RFE identifies these features as important, it confirms the relevance of domain expertise.\n","•\tEDA Corroboration: If prior EDA revealed certain features had high correlations with the target variable or demonstrated strong patterns in data visualization, RFE ranking might corroborate these findings by selecting those features as important.\n","•\tUnexpected Insights: RFE may also unveil features that were not immediately obvious in EDA but have predictive power according to the model. These findings could lead to further exploration and a deeper understanding of the problem.\n","2.\tFeature Importance from Tree-based Models: Feature importance from tree-based models quantifies the contribution of each feature to the model's performance. The interpretation can be as follows:\n","•\tDomain Knowledge Alignment: If the feature importance scores align with domain knowledge, it provides a clear confirmation of the domain expertise. For instance, if a feature known to be crucial is assigned high importance, it supports domain knowledge.\n","•\tEDA Validation: If EDA indicated that certain features played a significant role in the problem, and the feature importance scores reflect this, it validates the earlier observations made during exploratory data analysis.\n","•\tPrioritizing Unobserved Patterns: In some cases, feature importance may identify features that were not highlighted during EDA but are nonetheless crucial for the model. This can reveal hidden patterns in the data.\n","In both cases, it's important to compare the selected features with what is known about the problem. If the rankings align with domain knowledge or EDA, it provides a sense of confidence in the model's feature selection. If there are discrepancies, further investigation might be needed to understand why the model's perspective differs from human intuition.\n","Additionally, it's a good practice to assess the model's performance with the selected features to validate that the feature selection process has indeed improved the model's predictive power and generalization to unseen data."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOFhd8/Q8eQ1nYTzbqrELmT","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
